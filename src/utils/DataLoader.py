"""
æ™ºèƒ½æ•°æ®åŠ è½½å™¨
å‚è€ƒ PyTorch çš„ DataLoader è®¾è®¡æ€è·¯ï¼Œæä¾›æ™ºèƒ½çš„æ•°æ®åŠ è½½å’Œè‡ªåŠ¨ç±»å‹è¯†åˆ«åŠŸèƒ½
"""

import os
import pandas as pd
from typing import Dict, List, Optional, Any, Type, Union
from datetime import datetime
from .DataSet import (
    DataSet, AirportSpecialRequirementDataSet, AirportRestrictionDataSet,
    FlightRestrictionDataSet, FlightSpecialRequirementDataSet, 
    SectorSpecialRequirementDataSet
)
from ..types.constraint_models import (
    AirportSpecialRequirement, AirportRestriction, FlightRestriction,
    FlightSpecialRequirement, SectorSpecialRequirement,
    RestrictionType
)


class DatasetFactory:
    """æ•°æ®é›†å·¥å‚ç±»ï¼Œç”¨äºæ ¹æ®æ–‡ä»¶åè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ DataSet"""
    
    # æ–‡ä»¶ååˆ° DataSet ç±»çš„æ˜ å°„
    _dataset_mapping = {
        'airport_special_requirement': AirportSpecialRequirementDataSet,
        'airport_restriction': AirportRestrictionDataSet,
        'flight_restriction': FlightRestrictionDataSet,
        'flight_special_requirement': FlightSpecialRequirementDataSet,
        'sector_special_requirement': SectorSpecialRequirementDataSet,
    }
    
    @classmethod
    def create_dataset(cls, file_path: str):
        """
        æ ¹æ®æ–‡ä»¶è·¯å¾„è‡ªåŠ¨åˆ›å»ºå¯¹åº”çš„ DataSet
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¯¹åº”çš„ DataSet å®ä¾‹ï¼Œå¦‚æœæ— æ³•è¯†åˆ«åˆ™è¿”å› None
        """
        file_name = os.path.basename(file_path).lower()
        
        # å»é™¤æ–‡ä»¶æ‰©å±•å
        if '.' in file_name:
            file_name = file_name.rsplit('.', 1)[0]
            
        # æŸ¥æ‰¾åŒ¹é…çš„æ•°æ®é›†ç±»å‹
        for key, dataset_class in cls._dataset_mapping.items():
            if key == file_name:  # ç²¾ç¡®åŒ¹é…
                return dataset_class(file_path)
                
        print(f"è­¦å‘Š: æ— æ³•è¯†åˆ«æ–‡ä»¶ç±»å‹: {file_path} (æ–‡ä»¶å: {file_name})")
        return None
    
    @classmethod
    def get_supported_types(cls) -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶ç±»å‹åˆ—è¡¨"""
        return list(cls._dataset_mapping.keys())


class DataLoader:
    """æ™ºèƒ½æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_dir: str = "è¿è¡Œä¼˜åŒ–æ•°æ®", auto_load: bool = True):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_dir: CSVæ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼Œé»˜è®¤ä¸º"è¿è¡Œä¼˜åŒ–æ•°æ®"
            auto_load: æ˜¯å¦è‡ªåŠ¨åŠ è½½æ‰€æœ‰æ•°æ®ï¼Œé»˜è®¤ä¸º True
        """
        self.data_dir = data_dir
        self.datasets: Dict[str, DataSet] = {}
        self._cache: Dict[str, List[Any]] = {}
        
        if auto_load:
            self.auto_discover_and_load()
    
    def auto_discover_and_load(self) -> None:
        """è‡ªåŠ¨å‘ç°å¹¶åŠ è½½æ•°æ®ç›®å½•ä¸­çš„æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶"""
        if not os.path.exists(self.data_dir):
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return
            
        print("ğŸ” è‡ªåŠ¨å‘ç°æ•°æ®æ–‡ä»¶...")
        
        # æ‰«æç›®å½•ä¸­çš„ CSV æ–‡ä»¶
        csv_files = []
        for file_name in os.listdir(self.data_dir):
            if file_name.lower().endswith('.csv'):
                file_path = os.path.join(self.data_dir, file_name)
                csv_files.append(file_path)
        
        if not csv_files:
            print(f"âŒ åœ¨ç›®å½• {self.data_dir} ä¸­æœªæ‰¾åˆ° CSV æ–‡ä»¶")
            return
            
        print(f"ğŸ“ å‘ç° {len(csv_files)} ä¸ª CSV æ–‡ä»¶")
        
                 # è‡ªåŠ¨åŠ è½½æ¯ä¸ªæ–‡ä»¶
        loaded_count = 0
        for file_path in csv_files:
            dataset = DatasetFactory.create_dataset(file_path)
            if dataset is not None:
                try:
                    dataset.load_data()
                    dataset_key = self._get_dataset_key(dataset)
                    self.datasets[dataset_key] = dataset
                    loaded_count += 1
                    print(f"âœ… å·²åŠ è½½: {os.path.basename(file_path)} ({len(dataset)} æ¡è®°å½•)")
                except Exception as e:
                    print(f"âŒ åŠ è½½å¤±è´¥: {os.path.basename(file_path)} - {e}")
                    import traceback
                    traceback.print_exc()  # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯
            else:
                print(f"âš ï¸  è·³è¿‡æœªè¯†åˆ«çš„æ–‡ä»¶: {os.path.basename(file_path)}")
        
        print(f"ğŸ‰ æˆåŠŸåŠ è½½ {loaded_count} ä¸ªæ•°æ®é›†")
        self._update_cache()
    
    def _get_dataset_key(self, dataset: DataSet) -> str:
        """æ ¹æ®æ•°æ®é›†ç±»å‹ç”Ÿæˆé”®å"""
        type_mapping = {
            'AirportSpecialRequirementDataSet': 'airport_special_requirements',
            'AirportRestrictionDataSet': 'airport_restrictions',
            'FlightRestrictionDataSet': 'flight_restrictions',
            'FlightSpecialRequirementDataSet': 'flight_special_requirements',
            'SectorSpecialRequirementDataSet': 'sector_special_requirements',
        }
        return type_mapping.get(dataset.__class__.__name__, 'unknown')
    
    def _update_cache(self) -> None:
        """æ›´æ–°ç¼“å­˜ï¼Œæä¾›å‘åå…¼å®¹çš„æ¥å£"""
        self._cache.clear()
        for key, dataset in self.datasets.items():
            self._cache[key] = dataset.get_all_items()
    
    # å‘åå…¼å®¹çš„å±æ€§è®¿é—®
    @property
    def airport_special_requirements(self) -> List[AirportSpecialRequirement]:
        return self._cache.get('airport_special_requirements', [])
    
    @property
    def airport_restrictions(self) -> List[AirportRestriction]:
        return self._cache.get('airport_restrictions', [])
    
    @property
    def flight_restrictions(self) -> List[FlightRestriction]:
        return self._cache.get('flight_restrictions', [])
    
    @property
    def flight_special_requirements(self) -> List[FlightSpecialRequirement]:
        return self._cache.get('flight_special_requirements', [])
    
    @property
    def sector_special_requirements(self) -> List[SectorSpecialRequirement]:
        return self._cache.get('sector_special_requirements', [])
        
    def load_all_data(self) -> None:
        """åŠ è½½æ‰€æœ‰CSVæ•°æ®ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰"""
        if not self.datasets:
            self.auto_discover_and_load()
        else:
            total_count = sum(len(dataset) for dataset in self.datasets.values())
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼æ€»è®¡åŠ è½½ {total_count} æ¡çº¦æŸè®°å½•")
            print(f"   â”œâ”€â”€ æœºåœºç‰¹æ®Šè¦æ±‚: {len(self.airport_special_requirements)} æ¡")
            print(f"   â”œâ”€â”€ æœºåœºé™åˆ¶: {len(self.airport_restrictions)} æ¡")
            print(f"   â”œâ”€â”€ èˆªç­é™åˆ¶: {len(self.flight_restrictions)} æ¡")
            print(f"   â”œâ”€â”€ èˆªç­ç‰¹æ®Šè¦æ±‚: {len(self.flight_special_requirements)} æ¡")
            print(f"   â””â”€â”€ èˆªæ®µç‰¹æ®Šè¦æ±‚: {len(self.sector_special_requirements)} æ¡")
    
    def load_specific_dataset(self, file_path: str) -> Optional[DataSet]:
        """
        åŠ è½½æŒ‡å®šçš„æ•°æ®æ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŠ è½½çš„æ•°æ®é›†ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        dataset = DatasetFactory.create_dataset(file_path)
        if dataset:
            try:
                dataset.load_data()
                dataset_key = self._get_dataset_key(dataset)
                self.datasets[dataset_key] = dataset
                self._update_cache()
                print(f"âœ… æˆåŠŸåŠ è½½: {os.path.basename(file_path)} ({len(dataset)} æ¡è®°å½•)")
                return dataset
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥: {os.path.basename(file_path)} - {e}")
        return None
    
    def get_dataset(self, dataset_type: str) -> Optional[DataSet]:
        """
        è·å–æŒ‡å®šç±»å‹çš„æ•°æ®é›†
        
        Args:
            dataset_type: æ•°æ®é›†ç±»å‹
            
        Returns:
            å¯¹åº”çš„æ•°æ®é›†
        """
        return self.datasets.get(dataset_type)
    
    def get_all_datasets(self) -> Dict[str, DataSet]:
        """è·å–æ‰€æœ‰æ•°æ®é›†"""
        return self.datasets.copy()
    
    def filter_data(self, dataset_type: str, condition_func) -> List[Any]:
        """
        æ ¹æ®æ¡ä»¶ç­›é€‰æŒ‡å®šç±»å‹çš„æ•°æ®
        
        Args:
            dataset_type: æ•°æ®é›†ç±»å‹
            condition_func: ç­›é€‰æ¡ä»¶å‡½æ•°
            
        Returns:
            ç­›é€‰åçš„æ•°æ®åˆ—è¡¨
        """
        dataset = self.datasets.get(dataset_type)
        if dataset:
            return dataset.filter_by_condition(condition_func)
        return []

    # å‘åå…¼å®¹çš„åŠ è½½æ–¹æ³• - è¿™äº›æ–¹æ³•ç°åœ¨é€šè¿‡ auto_discover_and_load è‡ªåŠ¨å¤„ç†
    def load_airport_special_requirements(self) -> None:
        """åŠ è½½æœºåœºç‰¹æ®Šè¦æ±‚æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰"""
        file_path = os.path.join(self.data_dir, "airport_special_requirement.csv")
        if os.path.exists(file_path):
            self.load_specific_dataset(file_path)

    def load_airport_restrictions(self) -> None:
        """åŠ è½½æœºåœºé™åˆ¶æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰"""
        file_path = os.path.join(self.data_dir, "airport_restriction.csv")
        if os.path.exists(file_path):
            self.load_specific_dataset(file_path)

    def load_flight_restrictions(self) -> None:
        """åŠ è½½èˆªç­é™åˆ¶æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰"""
        file_path = os.path.join(self.data_dir, "flight_restriction.csv")
        if os.path.exists(file_path):
            self.load_specific_dataset(file_path)

    def load_flight_special_requirements(self) -> None:
        """åŠ è½½èˆªç­ç‰¹æ®Šè¦æ±‚æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰"""
        file_path = os.path.join(self.data_dir, "flight_special_requirement.csv")
        if os.path.exists(file_path):
            self.load_specific_dataset(file_path)

    def load_sector_special_requirements(self) -> None:
        """åŠ è½½èˆªæ®µç‰¹æ®Šè¦æ±‚æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰"""
        file_path = os.path.join(self.data_dir, "sector_special_requirement.csv")
        if os.path.exists(file_path):
            self.load_specific_dataset(file_path)

    def get_airport_curfew_restrictions(self, airport_code: str, check_time: datetime) -> List[AirportRestriction]:
        """è·å–æŒ‡å®šæœºåœºåœ¨æŒ‡å®šæ—¶é—´çš„å®µç¦é™åˆ¶"""
        restrictions = []
        for restriction in self.airport_restrictions:
            if (restriction.restriction_type == RestrictionType.AIRPORT_CURFEW and 
                restriction.airport_code == airport_code and 
                restriction.is_active(check_time)):
                restrictions.append(restriction)
        return restrictions

    def get_flight_restrictions(self, flight_no: str, carrier_code: str, 
                              dep_airport: str, arr_airport: Optional[str], 
                              check_time: datetime) -> List[FlightRestriction]:
        """è·å–æŒ‡å®šèˆªç­çš„é™åˆ¶"""
        restrictions = []
        for restriction in self.flight_restrictions:
            if restriction.applies_to_flight(flight_no, carrier_code, dep_airport, arr_airport, check_time):
                restrictions.append(restriction)
        return restrictions

    def get_airport_special_requirements(self, airport_code: str, check_time: datetime) -> List[AirportSpecialRequirement]:
        """è·å–æŒ‡å®šæœºåœºçš„ç‰¹æ®Šè¦æ±‚"""
        requirements = []
        for req in self.airport_special_requirements:
            if req.is_active(check_time, airport_code):
                requirements.append(req)
        return requirements

    def get_sector_special_requirements(self, dep_airport: str, arr_airport: str, 
                                      carrier_code: str, check_time: datetime) -> List[SectorSpecialRequirement]:
        """è·å–æŒ‡å®šèˆªæ®µçš„ç‰¹æ®Šè¦æ±‚"""
        requirements = []
        for req in self.sector_special_requirements:
            if req.applies_to_sector(dep_airport, arr_airport, carrier_code, check_time):
                requirements.append(req)
        return requirements

    def get_flight_special_requirements(self, flight_no: str, carrier_code: str, 
                                      dep_airport: str, arr_airport: str, 
                                      check_time: datetime) -> List[FlightSpecialRequirement]:
        """è·å–æŒ‡å®šèˆªç­çš„ç‰¹æ®Šè¦æ±‚"""
        requirements = []
        for req in self.flight_special_requirements:
            if req.applies_to_flight(flight_no, carrier_code, dep_airport, arr_airport, check_time):
                requirements.append(req)
        return requirements

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        # åŸºç¡€ç»Ÿè®¡
        stats = {
            "æ€»çº¦æŸæ•°": sum(len(dataset) for dataset in self.datasets.values()),
            "æœºåœºç‰¹æ®Šè¦æ±‚": len(self.airport_special_requirements),
            "æœºåœºé™åˆ¶": len(self.airport_restrictions),
            "èˆªç­é™åˆ¶": len(self.flight_restrictions),
            "èˆªç­ç‰¹æ®Šè¦æ±‚": len(self.flight_special_requirements),
            "èˆªæ®µç‰¹æ®Šè¦æ±‚": len(self.sector_special_requirements),
            "æ•°æ®é›†è¯¦æƒ…": {}
        }
        
        # æ·»åŠ æ¯ä¸ªæ•°æ®é›†çš„è¯¦ç»†ç»Ÿè®¡
        for key, dataset in self.datasets.items():
            dataset_stats = dataset.get_statistics()
            stats["æ•°æ®é›†è¯¦æƒ…"][key] = dataset_stats
        
        # è®¡ç®—ä¼˜å…ˆçº§åˆ†å¸ƒ
        priority_dist = {}
        all_constraints = (
            self.airport_special_requirements + 
            self.airport_restrictions + 
            self.flight_restrictions + 
            self.flight_special_requirements + 
            self.sector_special_requirements
        )
        
        for constraint in all_constraints:
            if hasattr(constraint, 'priority'):
                priority_name = constraint.priority.value if hasattr(constraint.priority, 'value') else str(constraint.priority)
                priority_dist[priority_name] = priority_dist.get(priority_name, 0) + 1
        
        stats["ä¼˜å…ˆçº§åˆ†å¸ƒ"] = priority_dist
        
        # è®¡ç®—åˆ†ç±»åˆ†å¸ƒ  
        category_dist = {}
        for constraint in all_constraints:
            if hasattr(constraint, 'category'):
                category_name = constraint.category.value if hasattr(constraint.category, 'value') else str(constraint.category)
                category_dist[category_name] = category_dist.get(category_name, 0) + 1
            elif hasattr(constraint, 'restriction_type'):
                # å¯¹äºæ²¡æœ‰categoryçš„çº¦æŸï¼Œä½¿ç”¨restriction_type
                type_name = constraint.restriction_type.value if hasattr(constraint.restriction_type, 'value') else str(constraint.restriction_type)
                category_dist[type_name] = category_dist.get(type_name, 0) + 1
        
        stats["åˆ†ç±»åˆ†å¸ƒ"] = category_dist
            
        return stats
    
    def get_supported_file_types(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶ç±»å‹"""
        return DatasetFactory.get_supported_types()
    
    def reload_data(self) -> None:
        """é‡æ–°åŠ è½½æ‰€æœ‰æ•°æ®"""
        self.datasets.clear()
        self._cache.clear()
        self.auto_discover_and_load()
    
    def add_custom_dataset(self, key: str, dataset: DataSet) -> None:
        """
        æ·»åŠ è‡ªå®šä¹‰æ•°æ®é›†
        
        Args:
            key: æ•°æ®é›†é”®å
            dataset: æ•°æ®é›†å®ä¾‹
        """
        self.datasets[key] = dataset
        self._update_cache()
    
    def export_to_pandas(self, dataset_type: str) -> Optional[pd.DataFrame]:
        """
        å°†æŒ‡å®šæ•°æ®é›†å¯¼å‡ºä¸º pandas DataFrame
        
        Args:
            dataset_type: æ•°æ®é›†ç±»å‹
            
        Returns:
            pandas DataFrameï¼Œå¦‚æœæ•°æ®é›†ä¸å­˜åœ¨åˆ™è¿”å› None
        """
        dataset = self.datasets.get(dataset_type)
        if dataset and hasattr(dataset, 'data'):
            return dataset.data.copy()
        return None
    
    def get_dataset_info(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯"""
        info = {}
        for key, dataset in self.datasets.items():
            info[key] = {
                "ç±»å‹": dataset.get_data_type().__name__,
                "æ–‡ä»¶è·¯å¾„": dataset.file_path,
                "è®°å½•æ•°": len(dataset),
                "ç»Ÿè®¡ä¿¡æ¯": dataset.get_statistics()
            }
        return info 